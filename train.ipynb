{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. LOAD DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T05:47:05.063141Z",
     "iopub.status.busy": "2020-11-26T05:47:05.063141Z",
     "iopub.status.idle": "2020-11-26T05:47:08.062110Z",
     "shell.execute_reply": "2020-11-26T05:47:08.061115Z",
     "shell.execute_reply.started": "2020-11-26T05:47:05.063141Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset bookcorpus (C:\\Users\\tmuds\\.cache\\huggingface\\datasets\\bookcorpus\\plain_text\\1.0.0\\af844be26c089fb64810e9f2cd841954fd8bd596d6ddd26326e4c70e2b8c96fc)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "dataset = load_dataset('bookcorpus', split='train')\n",
    "dataset.set_format('pandas', ['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. TRAIN TOKENIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-22T13:34:45.115872Z",
     "iopub.status.busy": "2020-11-22T13:34:45.115872Z",
     "iopub.status.idle": "2020-11-22T13:40:46.054026Z",
     "shell.execute_reply": "2020-11-22T13:40:46.053547Z",
     "shell.execute_reply.started": "2020-11-22T13:34:45.115872Z"
    }
   },
   "outputs": [],
   "source": [
    "from tokenizers.implementations import BertWordPieceTokenizer, ByteLevelBPETokenizer\n",
    "\n",
    "# Initialize a tokenizer\n",
    "tokenizer = ByteLevelBPETokenizer(lowercase=True)\n",
    "\n",
    "# Customize training\n",
    "tokenizer.train(files='bookcorpus.txt', vocab_size=50000, min_frequency=2, special_tokens=['<s>', '<pad>', '</s>', '<unk>', '<mask>'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-22T13:46:50.766467Z",
     "iopub.status.busy": "2020-11-22T13:46:50.766467Z",
     "iopub.status.idle": "2020-11-22T13:46:50.813340Z",
     "shell.execute_reply": "2020-11-22T13:46:50.812343Z",
     "shell.execute_reply.started": "2020-11-22T13:46:50.766467Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BonzLM\\\\vocab.json', 'BonzLM\\\\merges.txt']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_model(\"BonzLM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. INIT LANGUAGE MODELING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T05:47:15.147489Z",
     "iopub.status.busy": "2020-11-26T05:47:15.147489Z",
     "iopub.status.idle": "2020-11-26T05:47:15.171425Z",
     "shell.execute_reply": "2020-11-26T05:47:15.170427Z",
     "shell.execute_reply.started": "2020-11-26T05:47:15.147489Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast, RobertaTokenizerFast\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('./tokenzier/WordPiece-10k/', max_len=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T05:47:16.717380Z",
     "iopub.status.busy": "2020-11-26T05:47:16.717380Z",
     "iopub.status.idle": "2020-11-26T05:47:16.732337Z",
     "shell.execute_reply": "2020-11-26T05:47:16.731378Z",
     "shell.execute_reply.started": "2020-11-26T05:47:16.717380Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def create_position_ids_from_input_ids(input_ids, padding_idx):\n",
    "    \"\"\"\n",
    "    Replace non-padding symbols with their position numbers. Position numbers begin at padding_idx+1. Padding symbols\n",
    "    are ignored. This is modified from fairseq's `utils.make_positions`.\n",
    "\n",
    "    Args:\n",
    "        x: torch.Tensor x:\n",
    "\n",
    "    Returns: torch.Tensor\n",
    "    \"\"\"\n",
    "    # The series of casts and type-conversions here are carefully balanced to both work with ONNX export and XLA.\n",
    "    mask = input_ids.ne(padding_idx).int()\n",
    "    incremental_indices = torch.cumsum(mask, dim=1).type_as(mask) * mask\n",
    "    return incremental_indices.long() + padding_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reformer Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T05:47:18.699439Z",
     "iopub.status.busy": "2020-11-26T05:47:18.699439Z",
     "iopub.status.idle": "2020-11-26T05:47:18.727364Z",
     "shell.execute_reply": "2020-11-26T05:47:18.726366Z",
     "shell.execute_reply.started": "2020-11-26T05:47:18.699439Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ReformerConfig {\n",
       "  \"attention_head_size\": 64,\n",
       "  \"attn_layers\": [\n",
       "    \"local\",\n",
       "    \"lsh\",\n",
       "    \"local\",\n",
       "    \"lsh\",\n",
       "    \"local\",\n",
       "    \"lsh\"\n",
       "  ],\n",
       "  \"axial_norm_std\": 1.0,\n",
       "  \"axial_pos_embds\": true,\n",
       "  \"axial_pos_embds_dim\": [\n",
       "    64,\n",
       "    64\n",
       "  ],\n",
       "  \"axial_pos_shape\": [\n",
       "    16,\n",
       "    16\n",
       "  ],\n",
       "  \"bos_token_id\": 2,\n",
       "  \"chunk_size_lm_head\": 0,\n",
       "  \"eos_token_id\": 3,\n",
       "  \"feed_forward_size\": 512,\n",
       "  \"feed_foward_size\": 512,\n",
       "  \"hash_seed\": null,\n",
       "  \"hidden_act\": \"relu\",\n",
       "  \"hidden_dropout_prob\": 0.05,\n",
       "  \"hidden_size\": 128,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"local_attention_probs_dropout_prob\": 0.05,\n",
       "  \"local_attn_chunk_length\": 32,\n",
       "  \"local_num_chunks_after\": 0,\n",
       "  \"local_num_chunks_before\": 1,\n",
       "  \"lsh_attention_probs_dropout_prob\": 0.0,\n",
       "  \"lsh_attn_chunk_length\": 32,\n",
       "  \"lsh_num_chunks_after\": 0,\n",
       "  \"lsh_num_chunks_before\": 1,\n",
       "  \"max_position_embeddings\": 256,\n",
       "  \"model_type\": \"reformer\",\n",
       "  \"num_attention_heads\": 6,\n",
       "  \"num_buckets\": 8,\n",
       "  \"num_hashes\": 4,\n",
       "  \"num_hidden_layers\": 6,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"vocab_size\": 10000\n",
       "}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import ReformerConfig\n",
    "\n",
    "config = ReformerConfig(attention_head_size=64,\n",
    "                        num_attention_heads=6,\n",
    "                        num_hidden_layers=6,\n",
    "                        num_buckets=8,\n",
    "                        num_hashes=4,\n",
    "                        axial_pos_embds_dim=[64, 64],\n",
    "                        hidden_size=128,\n",
    "                        axial_pos_shape=[16, 16],\n",
    "                        max_position_embeddings= 256,\n",
    "                        bos_token_id=2,\n",
    "                        eos_token_id=3,\n",
    "                        pad_token_id=0,\n",
    "                        feed_foward_size=512,\n",
    "                        hash_seed=None,\n",
    "                        local_attn_chunk_length=32,\n",
    "                        lsh_attn_chunk_length=32,\n",
    "                        vocab_size=tokenizer.vocab_size\n",
    "                       )\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init Reformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T05:47:21.286099Z",
     "iopub.status.busy": "2020-11-26T05:47:21.286099Z",
     "iopub.status.idle": "2020-11-26T05:47:21.389819Z",
     "shell.execute_reply": "2020-11-26T05:47:21.388821Z",
     "shell.execute_reply.started": "2020-11-26T05:47:21.286099Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5678096"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import ReformerForMaskedLM\n",
    "\n",
    "model = ReformerForMaskedLM(config=config)\n",
    "model.num_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2020-11-21T18:50:32.523462Z",
     "iopub.status.busy": "2020-11-21T18:50:32.523462Z",
     "iopub.status.idle": "2020-11-21T18:50:39.129799Z",
     "shell.execute_reply": "2020-11-21T18:50:39.128802Z",
     "shell.execute_reply.started": "2020-11-21T18:50:32.523462Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-cea0a3c7ccc1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mbenchmark_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPyTorchBenchmarkArguments\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msequence_lengths\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m512\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1024\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Reformer\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmulti_process\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mbenchmark\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPyTorchBenchmark\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfigs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnew_config\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbenchmark_args\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbenchmark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\transformers\\benchmark\\benchmark_utils.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    696\u001b[0m                             \u001b[0minference_result_memory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"result\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmemory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    697\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspeed\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 698\u001b[1;33m                             \u001b[0mtime\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minference_speed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    699\u001b[0m                             \u001b[0minference_result_time\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"result\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    700\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\transformers\\benchmark\\benchmark_utils.py\u001b[0m in \u001b[0;36minference_speed\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    656\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minference_speed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 658\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mseparate_process_wrapper_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inference_speed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo_multi_processing\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    659\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    660\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtrain_speed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\transformers\\benchmark\\benchmark.py\u001b[0m in \u001b[0;36m_inference_speed\u001b[1;34m(self, model_name, batch_size, sequence_length)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_inference_speed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[0m_inference\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_prepare_inference_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_measure_speed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_inference\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     def _inference_memory(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\transformers\\benchmark\\benchmark.py\u001b[0m in \u001b[0;36m_measure_speed\u001b[1;34m(self, func)\u001b[0m\n\u001b[0;32m    209\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m                 \u001b[0mrepeat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m                 \u001b[0mnumber\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m             )\n\u001b[0;32m    213\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\timeit.py\u001b[0m in \u001b[0;36mrepeat\u001b[1;34m(stmt, setup, timer, repeat, number, globals)\u001b[0m\n\u001b[0;32m    235\u001b[0m            repeat=default_repeat, number=default_number, globals=None):\n\u001b[0;32m    236\u001b[0m     \u001b[1;34m\"\"\"Convenience function to create Timer object and call repeat method.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 237\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mTimer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstmt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msetup\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumber\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    238\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_wrap_timer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\timeit.py\u001b[0m in \u001b[0;36mrepeat\u001b[1;34m(self, repeat, number)\u001b[0m\n\u001b[0;32m    202\u001b[0m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m             \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumber\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m             \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\timeit.py\u001b[0m in \u001b[0;36mtimeit\u001b[1;34m(self, number)\u001b[0m\n\u001b[0;32m    174\u001b[0m         \u001b[0mgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 176\u001b[1;33m             \u001b[0mtiming\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    177\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mgcold\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\timeit.py\u001b[0m in \u001b[0;36minner\u001b[1;34m(_it, _timer, _stmt)\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\transformers\\benchmark\\benchmark.py\u001b[0m in \u001b[0;36mencoder_forward\u001b[1;34m()\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mencoder_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m                 \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minference_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\transformers\\modeling_reformer.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, position_ids, head_mask, inputs_embeds, num_hashes, past_buckets_states, use_cache, output_hidden_states, output_attentions, return_dict)\u001b[0m\n\u001b[0;32m   2088\u001b[0m             \u001b[0mposition_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2089\u001b[0m             \u001b[0minputs_embeds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2090\u001b[1;33m             \u001b[0mstart_idx_pos_encodings\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstart_idx_pos_encodings\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2091\u001b[0m         )\n\u001b[0;32m   2092\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\transformers\\modeling_reformer.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, position_ids, inputs_embeds, start_idx_pos_encodings)\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m         \u001b[1;31m# add positional embeddings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 264\u001b[1;33m         \u001b[0mposition_embeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mposition_embeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    265\u001b[0m         \u001b[0membeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0membeddings\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mposition_embeddings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0membeddings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\transformers\\modeling_reformer.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, position_ids)\u001b[0m\n\u001b[0;32m    199\u001b[0m                 [\n\u001b[0;32m    200\u001b[0m                     \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_select\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mposition_encodings\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mposition_ids\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m                     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m                 ],\n\u001b[0;32m    203\u001b[0m                 \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\transformers\\modeling_reformer.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    199\u001b[0m                 [\n\u001b[0;32m    200\u001b[0m                     \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_select\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mposition_encodings\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mposition_ids\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m                     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m                 ],\n\u001b[0;32m    203\u001b[0m                 \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import PyTorchBenchmarkArguments, PyTorchBenchmark\n",
    "\n",
    "new_config = config\n",
    "new_config.num_buckets = 16\n",
    "benchmark_args = PyTorchBenchmarkArguments(sequence_lengths=[256], batch_sizes=[16,32,64,128,256,512,1024], models=[\"Reformer\"], multi_process=False)\n",
    "benchmark = PyTorchBenchmark(configs=[new_config], args=benchmark_args,)\n",
    "result = benchmark.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T05:47:24.814041Z",
     "iopub.status.busy": "2020-11-26T05:47:24.813003Z",
     "iopub.status.idle": "2020-11-26T05:48:25.518162Z",
     "shell.execute_reply": "2020-11-26T05:48:25.518162Z",
     "shell.execute_reply.started": "2020-11-26T05:47:24.814041Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('bookcorpus.txt', names=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T05:48:25.519165Z",
     "iopub.status.busy": "2020-11-26T05:48:25.519165Z",
     "iopub.status.idle": "2020-11-26T05:48:25.883194Z",
     "shell.execute_reply": "2020-11-26T05:48:25.883194Z",
     "shell.execute_reply.started": "2020-11-26T05:48:25.519165Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   2, 1476, 2530,   17, 6739, 2491, 1633, 1497, 1476, 2497, 1495, 3121,\n",
       "        3204, 8052, 4837, 7228, 1506, 1015, 1577, 2201, 6235, 1011, 7559, 7185,\n",
       "        7228, 1506, 1015, 1577, 2201, 6235, 1011, 1597, 5411, 8684,   18, 1547,\n",
       "        1027, 1010,   30, 3668, 1033, 1042, 1033, 9398, 1044, 1034, 1032, 1547,\n",
       "        1027, 1010,   17, 3601,   30,   29, 8568,   17, 3668, 1033, 1042, 1033,\n",
       "        9398, 1044, 1034, 1034, 1533, 1559, 2411,   16, 1684, 9343, 1550, 1484,\n",
       "        1896, 2399, 4396, 1533, 1559, 4712, 2365,   21, 4602, 9564, 1013, 3543,\n",
       "        1484, 1501, 2645,   16, 1814,   35,   51, 3310,   51, 1561,   43, 2228,\n",
       "        2860, 1484, 1521, 2573,   18, 3338, 2102, 1013,   16, 1901, 4838, 1547,\n",
       "        1563, 1476, 2093, 1502, 1018, 3194, 1903, 1484, 3222,   18, 1968,   43,\n",
       "        2161, 2802, 2909,   16, 1476, 2321, 1839, 2476, 3365, 1649, 2338,   18,\n",
       "        1968,   43, 2093, 1839, 1649, 3075, 2547, 1014, 1778, 2247, 1638, 1502,\n",
       "        4415, 1599, 3182, 4287, 1623, 1541, 1649, 2625,   18, 1777, 1909, 2476,\n",
       "        2034, 1577, 3119,   16, 1577, 1639, 1028, 1017,   18, 1575, 1742, 1014,\n",
       "        1686, 1476, 2530, 1495, 1512,   18, 1565,   51, 1567,   18,   51, 1686,\n",
       "        1512, 1597, 1489, 3338, 2102, 1013, 1547, 1563, 1476, 2093, 1839, 1502,\n",
       "        1728, 1484, 1501, 1795, 2185,   18, 1476, 1777, 2607, 1925, 1612, 1633,\n",
       "        3365, 1581, 1547, 1909, 4996,   16, 6371,   16, 6780, 1489,   51, 1589,\n",
       "        2745, 1512, 1521, 1774,   18,   51, 2347, 2415, 1476, 3517, 2851, 2727,\n",
       "          16, 7748, 7801, 1484, 5077, 3590, 1517, 7843, 1013,   18,    3,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class BonzDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer):\n",
    "        super(BonzDataset, self).__init__()\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = self.tokenizer.encode(self.df['text'][idx], padding='max_length', truncation=True, return_tensors='pt').squeeze(0)\n",
    "        for i in range(1,1000):\n",
    "            concated_text = ' '.join(df['text'][idx: idx+i+1].tolist())\n",
    "            temp = self.tokenizer.encode(concated_text, padding='max_length', truncation=True, return_tensors='pt').squeeze(0)\n",
    "            if temp[-1] == self.tokenizer.pad_token_id:\n",
    "                input_ids = temp\n",
    "            else:\n",
    "                break\n",
    "        return input_ids\n",
    "\n",
    "dataset = BonzDataset(df, tokenizer)\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining DataCollator & Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T05:48:25.885188Z",
     "iopub.status.busy": "2020-11-26T05:48:25.885188Z",
     "iopub.status.idle": "2020-11-26T05:48:25.899148Z",
     "shell.execute_reply": "2020-11-26T05:48:25.899148Z",
     "shell.execute_reply.started": "2020-11-26T05:48:25.885188Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T05:50:14.905301Z",
     "iopub.status.busy": "2020-11-26T05:50:14.904305Z",
     "iopub.status.idle": "2020-11-26T05:50:16.362528Z",
     "shell.execute_reply": "2020-11-26T05:50:16.361523Z",
     "shell.execute_reply.started": "2020-11-26T05:50:14.905301Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "MAX_STEP = (100000 * 256) // BATCH_SIZE\n",
    "WARMUP_STEP = (MAX_STEP*0.05) // 1\n",
    "LEARNING_RATE = 5e-5\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./BonzLM\",\n",
    "    overwrite_output_dir=True,\n",
    "    # Training step\n",
    "    max_steps=MAX_STEP,\n",
    "    warmup_steps=WARMUP_STEP,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    save_steps=10000,\n",
    "    save_total_limit=2,\n",
    "    # mixed precision\n",
    "    fp16=True,\n",
    "    fp16_opt_level='O2',\n",
    "    seed=42,\n",
    "    # Learning rate setup\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=0.001,\n",
    "    adam_beta2=0.98,\n",
    "    adam_epsilon=1e-6,\n",
    "    max_grad_norm=0,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T05:50:19.283993Z",
     "iopub.status.busy": "2020-11-26T05:50:19.283993Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O2\n",
      "cast_model_type        : torch.float16\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : True\n",
      "master_weights         : True\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O2\n",
      "cast_model_type        : torch.float16\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : True\n",
      "master_weights         : True\n",
      "loss_scale             : dynamic\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
      "./BonzLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: duyduc1110 (use `wandb login --relogin` to force relogin)\n",
      "wandb: wandb version 0.10.11 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.5<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">./BonzLM</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/duyduc1110/huggingface\" target=\"_blank\">https://wandb.ai/duyduc1110/huggingface</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/duyduc1110/huggingface/runs/32d869x2\" target=\"_blank\">https://wandb.ai/duyduc1110/huggingface/runs/32d869x2</a><br/>\n",
       "                Run data is saved locally in <code>wandb\\run-20201126_135020-32d869x2</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tmuds\\miniconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:114: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='8394' max='100000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  8394/100000 8:02:40 < 87:48:52, 0.29 it/s, Epoch 0.03/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>9.264078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>9.264594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>9.264578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>9.264437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>9.264188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>9.264375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>9.264391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>9.264297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>9.264188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>9.264328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>9.264359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>9.264281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>9.264203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>9.264422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>9.264016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>9.264234</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-24T05:14:56.156742Z",
     "iopub.status.busy": "2020-11-24T05:14:56.155751Z",
     "iopub.status.idle": "2020-11-24T05:14:56.416489Z",
     "shell.execute_reply": "2020-11-24T05:14:56.414487Z",
     "shell.execute_reply.started": "2020-11-24T05:14:56.156742Z"
    }
   },
   "outputs": [],
   "source": [
    "trainer.save_model('test2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-24T05:14:53.882297Z",
     "iopub.status.busy": "2020-11-24T05:14:53.882297Z",
     "iopub.status.idle": "2020-11-24T05:14:53.897256Z",
     "shell.execute_reply": "2020-11-24T05:14:53.896258Z",
     "shell.execute_reply.started": "2020-11-24T05:14:53.882297Z"
    }
   },
   "outputs": [],
   "source": [
    "trainer.tokenizer = tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. TEST MODEL WITH MASK TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-24T05:27:34.149647Z",
     "iopub.status.busy": "2020-11-24T05:27:34.149647Z",
     "iopub.status.idle": "2020-11-24T05:27:34.673187Z",
     "shell.execute_reply": "2020-11-24T05:27:34.673187Z",
     "shell.execute_reply.started": "2020-11-24T05:27:34.149647Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "fill_mask = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=\"./test2\",\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-24T05:28:14.045852Z",
     "iopub.status.busy": "2020-11-24T05:28:14.045852Z",
     "iopub.status.idle": "2020-11-24T05:28:14.078762Z",
     "shell.execute_reply": "2020-11-24T05:28:14.077781Z",
     "shell.execute_reply.started": "2020-11-24T05:28:14.045852Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sequence': '<s>Today is a`` day.</s>',\n",
       "  'score': 0.2213928997516632,\n",
       "  'token': 308,\n",
       "  'token_str': '``'},\n",
       " {'sequence': '<s>Today is a very day.</s>',\n",
       "  'score': 0.0527605339884758,\n",
       "  'token': 832,\n",
       "  'token_str': 'very'},\n",
       " {'sequence': '<s>Today is athis day.</s>',\n",
       "  'score': 0.03341570124030113,\n",
       "  'token': 954,\n",
       "  'token_str': 'this'},\n",
       " {'sequence': '<s>Today is awhat day.</s>',\n",
       "  'score': 0.0221271850168705,\n",
       "  'token': 780,\n",
       "  'token_str': 'what'},\n",
       " {'sequence': '<s>Today is athe day.</s>',\n",
       "  'score': 0.01886608637869358,\n",
       "  'token': 350,\n",
       "  'token_str': 'the'}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask(\"Today is a <mask> day.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-24T06:29:58.475591Z",
     "iopub.status.busy": "2020-11-24T06:29:58.475591Z",
     "iopub.status.idle": "2020-11-24T06:30:03.706788Z",
     "shell.execute_reply": "2020-11-24T06:30:03.702799Z",
     "shell.execute_reply.started": "2020-11-24T06:29:58.475591Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_PROJECT=huggingface-demo\n",
      "env: GLUE_DIR=glue_data\n",
      "env: TASK_NAME=MRPC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/24/2020 14:30:01 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "11/24/2020 14:30:01 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='/tmp/$TASK_NAME/', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=32, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=0.0002, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='runs\\\\Nov24_14-30-01_DS3', logging_first_step=False, logging_steps=50, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=50, dataloader_num_workers=0, past_index=-1, run_name='/tmp/$TASK_NAME/', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None)\n",
      "Reusing dataset glue (C:\\Users\\tmuds\\.cache\\huggingface\\datasets\\glue\\mrpc\\1.0.0\\7c99657241149a24692c402a5c3f34d4c9f1df5ac2e4c3759fadea38f6cb29c4)\n",
      "[INFO|configuration_utils.py:411] 2020-11-24 14:30:03,415 >> loading configuration file ./test2\\config.json\n",
      "[INFO|configuration_utils.py:449] 2020-11-24 14:30:03,416 >> Model config ReformerConfig {\n",
      "  \"architectures\": [\n",
      "    \"ReformerForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_head_size\": 64,\n",
      "  \"attn_layers\": [\n",
      "    \"local\",\n",
      "    \"lsh\",\n",
      "    \"local\",\n",
      "    \"lsh\",\n",
      "    \"local\",\n",
      "    \"lsh\"\n",
      "  ],\n",
      "  \"axial_norm_std\": 1.0,\n",
      "  \"axial_pos_embds\": true,\n",
      "  \"axial_pos_embds_dim\": [\n",
      "    64,\n",
      "    64\n",
      "  ],\n",
      "  \"axial_pos_shape\": [\n",
      "    16,\n",
      "    16\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"chunk_size_lm_head\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"feed_forward_size\": 512,\n",
      "  \"feed_foward_size\": 512,\n",
      "  \"finetuning_task\": \"mrpc\",\n",
      "  \"hash_seed\": null,\n",
      "  \"hidden_act\": \"relu\",\n",
      "  \"hidden_dropout_prob\": 0.05,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"local_attention_probs_dropout_prob\": 0.05,\n",
      "  \"local_attn_chunk_length\": 32,\n",
      "  \"local_num_chunks_after\": 0,\n",
      "  \"local_num_chunks_before\": 1,\n",
      "  \"lsh_attention_probs_dropout_prob\": 0.0,\n",
      "  \"lsh_attn_chunk_length\": 32,\n",
      "  \"lsh_num_chunks_after\": 0,\n",
      "  \"lsh_num_chunks_before\": 1,\n",
      "  \"max_position_embeddings\": 256,\n",
      "  \"model_type\": \"reformer\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_buckets\": 16,\n",
      "  \"num_hashes\": 1,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:411] 2020-11-24 14:30:03,418 >> loading configuration file ./test2\\config.json\n",
      "[INFO|configuration_utils.py:449] 2020-11-24 14:30:03,419 >> Model config ReformerConfig {\n",
      "  \"architectures\": [\n",
      "    \"ReformerForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_head_size\": 64,\n",
      "  \"attn_layers\": [\n",
      "    \"local\",\n",
      "    \"lsh\",\n",
      "    \"local\",\n",
      "    \"lsh\",\n",
      "    \"local\",\n",
      "    \"lsh\"\n",
      "  ],\n",
      "  \"axial_norm_std\": 1.0,\n",
      "  \"axial_pos_embds\": true,\n",
      "  \"axial_pos_embds_dim\": [\n",
      "    64,\n",
      "    64\n",
      "  ],\n",
      "  \"axial_pos_shape\": [\n",
      "    16,\n",
      "    16\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"chunk_size_lm_head\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"feed_forward_size\": 512,\n",
      "  \"feed_foward_size\": 512,\n",
      "  \"hash_seed\": null,\n",
      "  \"hidden_act\": \"relu\",\n",
      "  \"hidden_dropout_prob\": 0.05,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"local_attention_probs_dropout_prob\": 0.05,\n",
      "  \"local_attn_chunk_length\": 32,\n",
      "  \"local_num_chunks_after\": 0,\n",
      "  \"local_num_chunks_before\": 1,\n",
      "  \"lsh_attention_probs_dropout_prob\": 0.0,\n",
      "  \"lsh_attn_chunk_length\": 32,\n",
      "  \"lsh_num_chunks_after\": 0,\n",
      "  \"lsh_num_chunks_before\": 1,\n",
      "  \"max_position_embeddings\": 256,\n",
      "  \"model_type\": \"reformer\",\n",
      "  \"num_attention_heads\": 6,\n",
      "  \"num_buckets\": 16,\n",
      "  \"num_hashes\": 1,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1577] 2020-11-24 14:30:03,419 >> Model name './test2' not found in model shortcut name list (google/reformer-crime-and-punishment). Assuming './test2' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "[INFO|tokenization_utils_base.py:1607] 2020-11-24 14:30:03,420 >> Didn't find file ./test2\\spiece.model. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1607] 2020-11-24 14:30:03,420 >> Didn't find file ./test2\\tokenizer.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1607] 2020-11-24 14:30:03,420 >> Didn't find file ./test2\\added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1648] 2020-11-24 14:30:03,421 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1648] 2020-11-24 14:30:03,421 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1648] 2020-11-24 14:30:03,421 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1648] 2020-11-24 14:30:03,421 >> loading file ./test2\\special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1648] 2020-11-24 14:30:03,422 >> loading file ./test2\\tokenizer_config.json\n",
      "Traceback (most recent call last):\n",
      "  File \"run_glue.py\", line 418, in <module>\n",
      "    main()\n",
      "  File \"run_glue.py\", line 242, in main\n",
      "    use_fast=model_args.use_fast_tokenizer,\n",
      "  File \"C:\\Users\\tmuds\\AppData\\Roaming\\Python\\Python37\\site-packages\\transformers\\tokenization_auto.py\", line 339, in from_pretrained\n",
      "    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n",
      "  File \"C:\\Users\\tmuds\\AppData\\Roaming\\Python\\Python37\\site-packages\\transformers\\tokenization_utils_base.py\", line 1653, in from_pretrained\n",
      "    resolved_vocab_files, pretrained_model_name_or_path, init_configuration, *init_inputs, **kwargs\n",
      "  File \"C:\\Users\\tmuds\\AppData\\Roaming\\Python\\Python37\\site-packages\\transformers\\tokenization_utils_base.py\", line 1671, in _from_pretrained\n",
      "    **(copy.deepcopy(kwargs)),\n",
      "  File \"C:\\Users\\tmuds\\AppData\\Roaming\\Python\\Python37\\site-packages\\transformers\\tokenization_utils_base.py\", line 1725, in _from_pretrained\n",
      "    tokenizer = cls(*init_inputs, **init_kwargs)\n",
      "  File \"C:\\Users\\tmuds\\AppData\\Roaming\\Python\\Python37\\site-packages\\transformers\\tokenization_reformer.py\", line 99, in __init__\n",
      "    self.sp_model.Load(vocab_file)\n",
      "  File \"C:\\Users\\tmuds\\miniconda3\\lib\\site-packages\\sentencepiece.py\", line 367, in Load\n",
      "    return self.LoadFromFile(model_file)\n",
      "  File \"C:\\Users\\tmuds\\miniconda3\\lib\\site-packages\\sentencepiece.py\", line 177, in LoadFromFile\n",
      "    return _sentencepiece.SentencePieceProcessor_LoadFromFile(self, arg)\n",
      "TypeError: not a string\n",
      "11/24/2020 14:30:03 - INFO - wandb.internal.internal -   Internal process exited\n"
     ]
    }
   ],
   "source": [
    "%env WANDB_PROJECT=huggingface-demo\n",
    "%env GLUE_DIR=glue_data\n",
    "%env TASK_NAME=MRPC\n",
    "\n",
    "!python run_glue.py \\\n",
    "  --model_name_or_path ./test2 \\\n",
    "  --task_name MRPC \\\n",
    "  --do_train \\\n",
    "  --do_eval \\\n",
    "  --max_seq_length 256 \\\n",
    "  --per_device_train_batch_size 32 \\\n",
    "  --learning_rate 2e-4 \\\n",
    "  --num_train_epochs 3 \\\n",
    "  --output_dir /tmp/$TASK_NAME/ \\\n",
    "  --overwrite_output_dir \\\n",
    "  --logging_steps 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-24T06:29:04.709924Z",
     "iopub.status.busy": "2020-11-24T06:29:04.709924Z",
     "iopub.status.idle": "2020-11-24T06:29:04.873757Z",
     "shell.execute_reply": "2020-11-24T06:29:04.872715Z",
     "shell.execute_reply.started": "2020-11-24T06:29:04.709924Z"
    }
   },
   "outputs": [],
   "source": [
    "trainer.model.save_pretrained('./test2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-24T06:29:34.184206Z",
     "iopub.status.busy": "2020-11-24T06:29:34.183210Z",
     "iopub.status.idle": "2020-11-24T06:29:34.296584Z",
     "shell.execute_reply": "2020-11-24T06:29:34.294589Z",
     "shell.execute_reply.started": "2020-11-24T06:29:34.183210Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./test2\\\\tokenizer_config.json',\n",
       " './test2\\\\special_tokens_map.json',\n",
       " './test2\\\\vocab.json',\n",
       " './test2\\\\merges.txt',\n",
       " './test2\\\\added_tokens.json')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained('./test2')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
